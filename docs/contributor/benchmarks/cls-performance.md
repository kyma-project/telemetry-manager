# Cluster Setup

Nodes:

- Type: beta.kubernetes.io/instance-type=n2-standard-8
- Count 10
- Memory: 32GB / node
- CPU: 8 Cores / node

### node details

```sh
➜ k get node -owide
NAME                                                     STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION       CONTAINER-RUNTIME
shoot--kyma-stage--d7ca774-cpu-worker-0-z1-68d7b-knt9m   Ready    <none>   22h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z1-68d7b-mkklm   Ready    <none>   22h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z1-68d7b-pf8lj   Ready    <none>   23h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z1-68d7b-vvdtj   Ready    <none>   23h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z2-68f79-bsvpx   Ready    <none>   23h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z2-68f79-gq4qb   Ready    <none>   22h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z2-68f79-nfbsp   Ready    <none>   23h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z3-5dc4c-jwm6j   Ready    <none>   22h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z3-5dc4c-mz8zd   Ready    <none>   23h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
shoot--kyma-stage--d7ca774-cpu-worker-0-z3-5dc4c-rnq2f   Ready    <none>   22h   v1.31.7   1.2.3.4       <none>        Garden Linux 1592.8   6.6.84-cloud-amd64   containerd://1.7.23
```

### cluster details

```sh
➜ kubectl version
Server Version: v1.31.7
```

### Log Pipeline

The test uses a basic LogPipeline with an enabled OTLP input and a default output to CLS:

[log-pipeline configuration](assets/otel-input.yaml)

# Goal

Validate if a default Telemetry module deployment can saturate one CLS-Instance (large).
Large instances support up to 30.000 logs per second if the logs are ingested using [OTLP](https://pages.github.tools.sap/perfx/cloud-logging-service/plans-and-prices/#storage-limitations).

# Direct Setup

The following setup establishes a baseline for the performance of telemetry gen. CLS is used as a sink for the logs generated by telemetry gen.

Log generation:

- Payload: ~2k (plus metadata)
- [telemetry-gen configuration](assets/telemetry-gen-direct-to-cls.yml)

```mermaid
flowchart LR
    A[telemetrygen] --> B[CLS-service]

    style A fill:#f9f,stroke:#333,stroke-width:1px
    style B fill:#bbf,stroke:#333,stroke-width:1px
```

(unit: logs per second)

| **workers ↓ / replicas →** | 1      | 5       | 10      |
|----------------------------|--------|---------|---------|
| **1**                      | ~475   | ~2.260  | ~4.600  |
| **5**                      | ~2.300 | ~11.600 | ~22.600 |
| **10**                     | ~4.500 | ~22.500 | ~43.600 |

## Result

To achieve more than 30000 logs per second, 10 replicas with 10 workers each are required.

# Telemetry Module Setup

The following setup measures the performance of the log gateway deployed using a LogPipeline of the Telemetry module. CLS is used as a sink in the configured LogPipeline. The LogPipeline is configured to use the OTLP input. During the test, we scale the number of replicas of the telemetry-log-gateway to measure their maximum throughput.

Log generation:

- Payload: ~2k (plus metadata)
- Workers: 10
- Replicas: 10
- [telemetry-gen configuration](assets/telemetry-gen-using-log-pipeline.yaml)

```mermaid
flowchart LR
    A[telemetrygen] --> C[telemetry-logs-gateway]
    C[telemetry-logs-gateway] --> B[CLS-service]

    style A fill:#f9f,stroke:#333,stroke-width:1px
    style B fill:#bbf,stroke:#333,stroke-width:1px
    style C fill:#bfb,stroke:#333,stroke-width:1px

```

Metrics in use:
- Throughput: sum(rate(otelcol_exporter_sent_log_records_total{}[$__rate_interval]))
- Enqueue failed: sum(rate(otelcol_exporter_enqueue_failed_log_records_total{}[$__rate_interval]))

| Gateway Replicas | Throughput (Msg/s) | Enqueue failed (Msg/s) | Memory per instance (MB) | CPU per instance | Bytes transmitted (MB/s) | Bytes received (MB/s) | istio Memory | istio cpu |
|------------------|--------------------|------------------------|--------------------------|------------------|--------------------------|-----------------------|--------------|-----------|
| 2                | 12.200             |                        | 100                      | 1,2              | 1,54                     | 28,6                  | 60           | 0,99      |
| 4                | 20.600             |                        | 110                      | 1,1              | 3,6                      | 48,0                  | 55           | 0,99      |
| 8                | 22.000             | 5.000                  | 1100                     | 0,9              | 6,2                      | 70,0                  | 56           | 0,99      |
|                  |                    |                        |                          |                  |                          |                       |              |           |

# Modified Telemetry Module Setup

The following setup measures the performance of the log gateway deployed using a LogPipeline of the Telemetry module. The LogPipeline is configured to use the OTLP input. During the test, we keep the number of replicas of the telemetry-log-gateway constant and increase the CPU limits of its Istio sidecar.

Log generation:

- Payload: ~2k (plus metadata)
- Workers: 10
- Replicas: 10
- [telemetry-gen configuration](assets/telemetry-gen-using-log-pipeline.yaml)

```mermaid
flowchart LR
    A[telemetrygen] --> C[telemetry-logs-gateway]
    C[telemetry-logs-gateway] --> B[CLS-service]

    style A fill:#f9f,stroke:#333,stroke-width:1px
    style B fill:#bbf,stroke:#333,stroke-width:1px
    style C fill:#bfb,stroke:#333,stroke-width:1px

```

Istio CPU limits are increased using the `sidecar.istio.io/proxyCPULimit: "<limits>"` annotation.

Gateway Replicas: 2

Metrics in use: 
- Throughput: sum(rate(otelcol_exporter_sent_log_records_total{}[$__rate_interval]))
- Enqueue failed: sum(rate(otelcol_exporter_enqueue_failed_log_records_total{}[$__rate_interval]))

| Istio Limits                                 | Throughput (Msg/s) | Enqueue failed (Msg/s) | Memory per instance (MB) | CPU per instance | Bytes transmitted (MB/s) | Bytes received (MB/s) | istio Memory | istio cpu |
|----------------------------------------------|--------------------|------------------------|--------------------------|------------------|--------------------------|-----------------------|--------------|-----------|
| 1                                            | 12.200             |                        | 100                      | 1,2              | 1,54                     | 28,6                  | 60           | 0,99      |
| 2                                            | 20.000             |                        | 220                      | 2,2              | 2,70                     | 46,4                  | 60           | 1,7       |
| 3                                            | ~21.000            | ~1.600                 | 1.200                    | 2,5              | 3,4                      | 56,3                  | 60           | 2,5       |
| unlimited (`sidecar.istio.io/proxyCPU: "2"`) | ~21.000            | ~3.500                 | 1.000                    | 2,7              | 3,5                      | 56,0                  | 67           | 3,4       |

# Results

The tested CLS instance can to handle ~20.000 log records per second.
This performance is achieved with 2 replicas of the telemetry-log-gateway and Istio sidecar with CPU limits increased to 2.0.
Similar performance can be achieved with 4 replicas of the telemetry-log-gateway and no increased Istio sidecar CPU limits.

Higher numbers of generated logs lead to enqueuing issues on the log-gateway side. This is caused by CLS rejecting the logs.
