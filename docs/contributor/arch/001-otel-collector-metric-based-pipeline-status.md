# 1. Trace/Metric Pipeline status based on Otel Collector metrics

Date: 2023-06-11

## Status

Proposed

## Context

The telemetry pipelines deploy active components to a cluster, responsible for dispatching data to third-party services. 
Inevitably, interruptions can occur, potentially hindering the successful delivery of data. 
To enhance short-term resilience, we implement retries and buffering mechanisms. 
Nevertheless, there may be scenarios where data cannot be delivered, requiring user notification for prompt action.

The typical situations causing problems are:

* Connectivity problems with the third-party service.
* Backpressure or throttling due to overloading the third-party service.
* Reaching the ingestion limit of a pipeline, which may occur even with auto-scaling in place (subject to a maximum replica setting).

These situations can be monitored by collecting metrics from the relevant components, with [recently added documentation](https://github.com/kyma-project/telemetry-manager/pull/423) to guide the process. 
However, this approach can be cumbersome for users, as it requires them to identify which services to scrape and filter the relevant metrics. 
Additionally, these details delve into internal aspects that may undergo changes in the future.

### Goal
* The telemetry manager handles the lifecycle of pipelines and should be the only entity responsible for interpreting the metrics generated by the components.
In the event of any problematic situation, the telemetry manager should promptly report it by marking the pipeline and module status as a warning. This way, users can easily detect and address any issues.
* Additionally, we aim to make custom metrics accessible through a dedicated endpoint, providing users with only the pertinent metrics. This endpoint will be maintained over the long term, even as internal configurations may change.
* Another effective means of notifying users about problems is by emitting Kubernetes (k8s) events when a pipeline transitions into an unhealthy mode.

### Questions
1. Should we consider introducing Prometheus as a short-term, non-persistent metric storage solution, or would it be more advisable to handle the metric scraping internally? 
If we opt for Prometheus, what's the best approach: sidecar deployment alongside existing services or a standalone deployment?
2. When it comes to integrating with the control loop for managing resources, we want to avoid active calls by the pipeline controllers, as this could potentially slow down the reconciliation process. 
Is it feasible to create a dedicated controller responsible for metric scraping and storing the results into a dummy Custom Resource Definition (CRD)?
3. Can we repurpose Prometheus alerts for monitoring status changes, and is it possible to implement a push-based approach for this purpose?

## Decision

### Metric Analysis

In the context of metric analysis, let's consider the following scenarios:

#### Direct Endpoint Scraping by the Operator (Without TSDB)

While direct endpoint scraping is the simplest approach and doesn't require Prometheus integration, it comes with a lot of limitations. 
When scraping an endpoint, you receive one value per timestamp, which may suffice for gauges. However, even for commonly used metrics like rates, you would require multiple values aggregated over time. 
Additionally, Prometheus automatically handles adjustments for breaks in monotonicity, such as counter resets resulting from target restarts. Manual scraping lacks this crucial feature.

#### Prometheus as a Sidecar

Now that we understand the necessity of using Prometheus for even basic metric analysis, let's explore how we can deploy it. 
One deployment approach involves deploying Prometheus as a sidecar alongside the operator. 
This method remains lightweight, as it doesn't necessitate any changes in the operator's code and can be easily achieved only using kustomize. 
However, this static setup does come with its drawbacks:

* It relies on a static configuration with no room for parameterization, limiting flexibility.
* Fine-tuning Prometheus dynamically based on the number of pipelines or other factors is not feasible.
* Sidecar is deployed even when there are no pipelines, which may lead to unnecessary resource utilization.

#### Standalone Prometheus Deployment

An alternative approach would involve embedding Prometheus within the code of the operator, similar to how we deploy Fluent Bit or Otel Collector. 
This approach grants you complete control and maximum flexibility in managing Prometheus.
It's equally important to communicate to the end user that this internal Prometheus is intended exclusively for internal purposes and should not be utilized for monitoring user workloads. 
Clear communication regarding its internal nature is crucial to avoid any misuses or misunderstandings.

**Despite its inherent complexity, our preference leans toward running Prometheus as a standalone deployment. This approach offers the highest level of flexibility and ensures future-proofing, making it the most robust choice for our needs.**

### Integrating Prometheus Querying into Operator Reconciliation Loop

#### Predefined PromQL Queries:
* The operator manages a set of predefined PromQL queries and communicates with Prometheus using [expression queries API](https://prometheus.io/docs/prometheus/latest/querying/api/#expression-queries) to collect data points. 
  It then evaluates the Pipeline Custom Resource (CR) statuses based on this data.
* The potential issue of long-running queries, which could hinder reconciliation, is addressed by introducing a separate monitoring Custom Resource (CR) with its own reconciliation loop.
  This separation effectively isolates the query execution and result storage from the tracepipeline and metricpipeline controllers, ensuring a smoother and more efficient workflow.
  It's worth noting that while this approach enhances efficiency, it can introduce increased complexity to the overall setup.

![Prometheus Integration with Additional Controller](../assets/prom-integration-extra-ctrl-flow.svg "Prometheus Integration with Additional Controller")

#### Prometheus Alerts:
* PromQL expressions are defined as Prometheus alerts, with Prometheus responsible for evaluating them.
* The operator uses the alerts API to periodically fetch information about active alerts.
* This method avoids potentially long-running queries that can hinder reconciliation by offloading evaluation to Prometheus and simply polling the results using [alerts API](https://prometheus.io/docs/prometheus/latest/querying/api/#alerts).
* Another advantages of this approach include clear separation of code and PromQL expressions.
* Additionally, it provides better troubleshooting capabilities as Prometheus state can be observed using the dashboard.

![Prometheus Integration using Alerts](../assets/prom-integration-alerts-flow.svg "Prometheus Integration using Alerts")

## Consequences


