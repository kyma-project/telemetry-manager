# 1. Trace/Metric Pipeline status based on Otel Collector metrics

Date: 2023-06-11

## Status

Proposed

## Context

The telemetry pipelines deploy active components to a cluster, responsible for dispatching data to third-party services. 
Inevitably, interruptions can occur, potentially hindering the successful delivery of data. 
To enhance short-term resilience, we implement retries and buffering mechanisms. 
Nevertheless, there may be scenarios where data cannot be delivered, requiring user notification for prompt action.

The typical situations causing problems are:

* Connectivity problems with the third-party service.
* Backpressure or throttling due to overloading the third-party service.
* Reaching the ingestion limit of a pipeline, which may occur even with auto-scaling in place (subject to a maximum replica setting).

These situations can be monitored by collecting metrics from the relevant components, with [recently added documentation](https://github.com/kyma-project/telemetry-manager/pull/423) to guide the process. 
However, this approach can be cumbersome for users, as it requires them to identify which services to scrape and filter the relevant metrics. 
Additionally, these details delve into internal aspects that may undergo changes in the future.

### Goal
* The telemetry manager handles the lifecycle of pipelines and should be the only entity responsible for interpreting the metrics generated by the components.
In the event of any problematic situation, the telemetry manager should promptly report it by marking the pipeline and module status as a warning. This way, users can easily detect and address any issues.
* Additionally, we aim to make custom metrics accessible through a dedicated endpoint, providing users with only the pertinent metrics. This endpoint will be maintained over the long term, even as internal configurations may change.
* Another effective means of notifying users about problems is by emitting Kubernetes (k8s) events when a pipeline transitions into an unhealthy mode.

### Questions
1. Should we consider introducing Prometheus as a short-term, non-persistent metric storage solution, or would it be more advisable to handle the metric scraping internally? 
If we opt for Prometheus, what's the best approach: sidecar deployment alongside existing services or a standalone deployment?
2. When it comes to integrating with the control loop for managing resources, we want to avoid active calls by the pipeline controllers, as this could potentially slow down the reconciliation process. 
Is it feasible to create a dedicated controller responsible for metric scraping and storing the results into a dummy Custom Resource Definition (CRD)?
3. Can we repurpose Prometheus alerts for monitoring status changes, and is it possible to implement a push-based approach for this purpose?

## Decision



## Consequences


